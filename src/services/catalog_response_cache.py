"""
Catalog Response Caching Service

Caches complete catalog API responses in Redis to minimize database queries
and improve response times.

This service implements aggressive caching with smart invalidation to achieve:
- 95%+ cache hit rates
- Sub-10ms response times for cached requests
- 90%+ reduction in database load

Usage:
    from src.services.catalog_response_cache import (
        get_cached_catalog_response,
        cache_catalog_response,
        invalidate_catalog_cache,
    )

    # Check cache first
    cached = await get_cached_catalog_response(gateway, params)
    if cached:
        return cached

    # Cache miss - fetch and cache
    result = await fetch_from_database(...)
    await cache_catalog_response(gateway, params, result)
    return result
"""

import json
import hashlib
import logging
from typing import Any
from datetime import datetime, timezone

from src.config.redis_config import get_redis_client

logger = logging.getLogger(__name__)

# Cache configuration
CATALOG_CACHE_TTL = 300  # 5 minutes - balance freshness vs performance
CATALOG_CACHE_PREFIX = "catalog:v2:"  # Version prefix for easy invalidation
CATALOG_METADATA_KEY = "catalog:metadata"


def get_catalog_cache_key(gateway: str | None, params: dict) -> str:
    """
    Generate deterministic cache key from request parameters.

    The cache key includes all parameters that affect the response to ensure
    correct cache hits. Parameters are hashed to keep Redis keys short.

    Args:
        gateway: Gateway filter (e.g., 'openrouter', 'all', None)
        params: Query parameters (limit, offset, filters, etc.)

    Returns:
        Cache key string (e.g., "catalog:v2:all:a3f5b2c1")

    Examples:
        >>> get_catalog_cache_key("openrouter", {"limit": 100, "offset": 0})
        'catalog:v2:openrouter:8f6e...'
        >>> get_catalog_cache_key(None, {"limit": 100})
        'catalog:v2:all:9a2b...'
    """
    # Normalize gateway value
    gateway_key = gateway or "all"

    # Build cache data with all relevant parameters
    cache_data = {
        "gateway": gateway_key,
        "limit": params.get("limit", 100),
        "offset": params.get("offset", 0),
        "include_huggingface": params.get("include_huggingface", False),
        "unique_models": params.get("unique_models", False),
        # Add other parameters as needed
    }

    # Hash parameters to keep keys short while maintaining uniqueness
    param_hash = hashlib.md5(
        json.dumps(cache_data, sort_keys=True).encode()
    ).hexdigest()[:8]  # Use first 8 chars for readability

    return f"{CATALOG_CACHE_PREFIX}{gateway_key}:{param_hash}"


async def get_cached_catalog_response(
    gateway: str | None,
    params: dict
) -> dict[str, Any] | None:
    """
    Retrieve cached catalog response if available.

    Args:
        gateway: Gateway filter ('openrouter', 'anthropic', etc.)
        params: Request parameters used to generate cache key

    Returns:
        Cached response dict with metadata, or None if cache miss

    Cache Hit Flow:
        1. Generate cache key from params
        2. Check Redis for key
        3. If found, deserialize JSON and return
        4. Track metrics (cache hit)

    Cache Miss Flow:
        1. Generate cache key from params
        2. Redis returns None
        3. Track metrics (cache miss)
        4. Caller should fetch data and cache it
    """
    try:
        redis = get_redis_client()
        if not redis:
            logger.debug("Redis not available - cache miss (no client)")
            _track_cache_miss(gateway)
            return None

        cache_key = get_catalog_cache_key(gateway, params)
        cached_data = redis.get(cache_key)

        if cached_data:
            logger.info(f"âœ… Cache HIT: {cache_key}")
            _track_cache_hit(gateway)

            # Deserialize and return
            return json.loads(cached_data)

        logger.debug(f"Cache MISS: {cache_key}")
        _track_cache_miss(gateway)
        return None

    except json.JSONDecodeError as e:
        logger.error(f"Cache data corrupted for key {cache_key}: {e}")
        _track_cache_miss(gateway)
        return None
    except Exception as e:
        logger.warning(f"Cache read failed: {e}")
        _track_cache_miss(gateway)
        return None


async def cache_catalog_response(
    gateway: str | None,
    params: dict,
    response: dict[str, Any],
    ttl: int = CATALOG_CACHE_TTL
) -> bool:
    """
    Cache catalog response in Redis.

    Args:
        gateway: Gateway filter
        params: Request parameters used to generate cache key
        response: Response data to cache (will be JSON serialized)
        ttl: Time-to-live in seconds (default: 300 = 5 minutes)

    Returns:
        True if cached successfully, False otherwise

    Implementation Notes:
        - Adds cache metadata (_cached_at, _cache_ttl) to response
        - Uses Redis SETEX for atomic set-with-expiry
        - Gracefully handles Redis failures (returns False)
        - Updates cache metadata for monitoring
    """
    try:
        redis = get_redis_client()
        if not redis:
            logger.debug("Redis not available - skip caching")
            return False

        cache_key = get_catalog_cache_key(gateway, params)

        # Add cache metadata to response
        response_with_meta = {
            **response,
            "_cached_at": datetime.now(timezone.utc).isoformat(),
            "_cache_ttl": ttl,
            "_cache_key": cache_key,
        }

        # Serialize to JSON
        try:
            serialized = json.dumps(response_with_meta)
        except TypeError as e:
            logger.error(f"Failed to serialize response for caching: {e}")
            return False

        # Store in Redis with TTL
        redis.setex(cache_key, ttl, serialized)

        logger.info(f"ðŸ’¾ Cached response: {cache_key} (TTL: {ttl}s, size: {len(serialized)} bytes)")

        # Update cache metadata for monitoring
        _update_cache_metadata(redis, gateway, len(serialized))

        return True

    except Exception as e:
        logger.warning(f"Cache write failed: {e}")
        return False


def invalidate_catalog_cache(gateway: str | None = None) -> int:
    """
    Invalidate catalog cache entries.

    This should be called after model sync operations to ensure fresh data.

    Args:
        gateway: If specified, only invalidate caches for this gateway.
                 If None, invalidate ALL catalog caches (use with caution).

    Returns:
        Number of cache keys deleted

    Examples:
        >>> invalidate_catalog_cache("openrouter")  # Invalidate OpenRouter only
        15
        >>> invalidate_catalog_cache()  # Invalidate all gateways
        250

    Performance Notes:
        - Uses SCAN instead of KEYS to avoid blocking Redis
        - Batch deletes keys for efficiency
        - Safe to call even if Redis unavailable
    """
    try:
        redis = get_redis_client()
        if not redis:
            logger.debug("Redis not available - skip cache invalidation")
            return 0

        if gateway:
            # Invalidate specific gateway
            pattern = f"{CATALOG_CACHE_PREFIX}{gateway}:*"
        else:
            # Invalidate all catalog caches
            pattern = f"{CATALOG_CACHE_PREFIX}*"

        # Use SCAN instead of KEYS to avoid blocking Redis
        # SCAN is cursor-based and won't block other operations
        deleted_count = 0
        cursor = 0

        while True:
            cursor, keys = redis.scan(cursor, match=pattern, count=100)

            if keys:
                # Batch delete for efficiency
                redis.delete(*keys)
                deleted_count += len(keys)

            if cursor == 0:
                break

        if deleted_count > 0:
            logger.info(
                f"ðŸ—‘ï¸  Invalidated {deleted_count} cache entries "
                f"(pattern: {pattern})"
            )
        else:
            logger.debug(f"No cache entries to invalidate (pattern: {pattern})")

        return deleted_count

    except Exception as e:
        logger.error(f"Cache invalidation failed: {e}")
        return 0


def get_cache_stats(gateway: str | None = None) -> dict[str, Any]:
    """
    Get cache statistics for monitoring.

    Args:
        gateway: Filter stats for specific gateway, or None for all

    Returns:
        Dictionary with cache statistics:
        - total_cached: Number of items cached
        - last_cached_at: Timestamp of last cache write
        - hit_rate: Cache hit rate percentage (if metrics available)
    """
    try:
        redis = get_redis_client()
        if not redis:
            return {"error": "Redis not available"}

        gateway_key = gateway or "all"
        metadata_key = f"{CATALOG_METADATA_KEY}:{gateway_key}"

        metadata = redis.hgetall(metadata_key)

        if not metadata:
            return {
                "gateway": gateway_key,
                "total_cached": 0,
                "last_cached_at": None,
            }

        return {
            "gateway": gateway_key,
            "total_cached": int(metadata.get("total_cached", 0)),
            "last_cached_at": metadata.get("last_cached_at"),
            "total_size_bytes": int(metadata.get("total_size_bytes", 0)),
        }

    except Exception as e:
        logger.error(f"Failed to get cache stats: {e}")
        return {"error": str(e)}


# ==================== Private Helper Functions ====================


def _track_cache_hit(gateway: str | None):
    """Track cache hit in Prometheus metrics"""
    try:
        from src.services.prometheus_metrics import catalog_cache_hits
        catalog_cache_hits.labels(gateway=gateway or "all").inc()
    except ImportError:
        # Metrics not available - not critical
        pass
    except Exception as e:
        logger.debug(f"Failed to track cache hit: {e}")


def _track_cache_miss(gateway: str | None):
    """Track cache miss in Prometheus metrics"""
    try:
        from src.services.prometheus_metrics import catalog_cache_misses
        catalog_cache_misses.labels(gateway=gateway or "all").inc()
    except ImportError:
        # Metrics not available - not critical
        pass
    except Exception as e:
        logger.debug(f"Failed to track cache miss: {e}")


def _update_cache_metadata(redis, gateway: str | None, size_bytes: int):
    """
    Update cache metadata for monitoring.

    Stores statistics about cache usage:
    - total_cached: Incremented on each cache write
    - last_cached_at: Timestamp of last write
    - total_size_bytes: Cumulative size of cached data
    """
    try:
        gateway_key = gateway or "all"
        metadata_key = f"{CATALOG_METADATA_KEY}:{gateway_key}"

        # Use pipeline for atomic updates
        pipe = redis.pipeline()
        pipe.hincrby(metadata_key, "total_cached", 1)
        pipe.hincrby(metadata_key, "total_size_bytes", size_bytes)
        pipe.hset(metadata_key, "last_cached_at", datetime.now(timezone.utc).isoformat())
        pipe.expire(metadata_key, 86400)  # Metadata expires after 24 hours
        pipe.execute()

    except Exception as e:
        # Non-critical - don't fail cache operation
        logger.debug(f"Failed to update cache metadata: {e}")